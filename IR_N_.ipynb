{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n25tQ8kVY4m"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers matplotlib seaborn gensim scikit-learn nltk wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import reuters, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "# Vectorization & Clustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, normalized_mutual_info_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ZoePLviDVdDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "metadata": {
        "id": "6w-SPOaAVgUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "id": "5dCGg8o3ViV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ],
      "metadata": {
        "id": "wB2GqHwbVlAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "def summarize_cluster(text, sentences=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LexRankSummarizer()\n",
        "    summary_sentences = summarizer(parser.document, sentences)\n",
        "    return \" \".join([str(s) for s in summary_sentences])"
      ],
      "metadata": {
        "id": "NhZtOZfHVnSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    sbert_available = True\n",
        "except Exception:\n",
        "    sbert_available = False"
      ],
      "metadata": {
        "id": "K9cJbmP0Vpm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_reuters_dataset(max_docs: Optional[int] = 1000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load Reuters dataset from NLTK into a DataFrame with columns:\n",
        "    - doc_id\n",
        "    - title (first sentence or truncated first 10 words)\n",
        "    - text\n",
        "    - categories (list)\n",
        "    \"\"\"\n",
        "    fileids = reuters.fileids()\n",
        "    texts = []\n",
        "    for fid in fileids[:max_docs]:\n",
        "        raw = reuters.raw(fid)\n",
        "        # Create a short title from first sentence or first 10 words\n",
        "        sents = sent_tokenize(raw)\n",
        "        if len(sents) > 0:\n",
        "            title = sents[0].strip()\n",
        "            if len(title.split()) > 12:\n",
        "                title = \" \".join(title.split()[:12]) + \"...\"\n",
        "        else:\n",
        "            title = \"Reuters article \" + fid\n",
        "        cats = reuters.categories(fid)\n",
        "        texts.append({'doc_id': fid, 'title': title, 'text': raw, 'categories': cats})\n",
        "    df = pd.DataFrame(texts)\n",
        "    print(f\"Loaded {len(df)} Reuters documents.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "jXb37gOYVsQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "ihkES7NrVvNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text: str,\n",
        "                    remove_numbers: bool = True,\n",
        "                    remove_punct: bool = True,\n",
        "                    do_lemmatize: bool = True) -> str:\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    if remove_numbers:\n",
        "        text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    if remove_punct:\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    tokens = [t for t in tokens if len(t) > 2 and t not in STOPWORDS]\n",
        "\n",
        "    if do_lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame, text_col: str = 'text', new_col: str = 'clean_text') -> pd.DataFrame:\n",
        "    df[new_col] = df[text_col].apply(preprocess_text)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "dAYdoprJVy4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(docs, max_features=5000, ngram_range=(1,2)):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        ngram_range=ngram_range\n",
        "    )\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    return vectorizer, vectors\n"
      ],
      "metadata": {
        "id": "qCeDoEyeV2Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kmeans(vectors, k=5, random_state=42):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
        "    labels = kmeans.fit_predict(vectors)\n",
        "    return kmeans, labels\n",
        "def run_dbscan(X, eps=0.8, min_samples=5):\n",
        "\n",
        "    if hasattr(X, \"toarray\"):\n",
        "        X = X.toarray()\n",
        "\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(X)\n",
        "    return dbscan, labels\n"
      ],
      "metadata": {
        "id": "SFSPP5mEV_ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_tfidf_terms_per_cluster(tfidf_matrix, labels, vectorizer, top_n: int = 8) -> Dict[int, List[Tuple[str, float]]]:\n",
        "    terms = np.array(vectorizer.get_feature_names_out())\n",
        "    labels = np.array(labels)\n",
        "    cluster_terms = {}\n",
        "\n",
        "    for cluster in sorted(set(labels)):\n",
        "\n",
        "        mask = (labels == cluster)\n",
        "        if np.sum(mask) == 0:\n",
        "            cluster_terms[cluster] = []\n",
        "            continue\n",
        "\n",
        "\n",
        "        cluster_vec = tfidf_matrix[mask].mean(axis=0)\n",
        "\n",
        "\n",
        "        if hasattr(cluster_vec, \"A1\"):\n",
        "            cluster_vec = np.squeeze(np.asarray(cluster_vec))\n",
        "\n",
        "\n",
        "        top_idx = np.argsort(cluster_vec)[-top_n:][::-1]\n",
        "        top_terms = [(terms[i], float(cluster_vec[i])) for i in top_idx]\n",
        "\n",
        "        cluster_terms[cluster] = top_terms\n",
        "\n",
        "    return cluster_terms"
      ],
      "metadata": {
        "id": "C7xj4nmxWLLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "def top_tfidf_terms_per_cluster(tfidf_matrix, labels, vectorizer, top_n: int = 8) -> Dict[int, List[Tuple[str, float]]]:\n",
        "    terms = np.array(vectorizer.get_feature_names_out())\n",
        "    labels = np.array(labels)\n",
        "    cluster_terms = {}\n",
        "\n",
        "    for cluster in sorted(set(labels)):\n",
        "        mask = (labels == cluster)\n",
        "\n",
        "        if np.sum(mask) == 0:\n",
        "            cluster_terms[cluster] = []\n",
        "            continue\n",
        "\n",
        "        cluster_vec = tfidf_matrix[mask].mean(axis=0)\n",
        "\n",
        "        # Convert sparse matrix to dense array\n",
        "        if hasattr(cluster_vec, \"A1\"):\n",
        "            cluster_vec = np.squeeze(np.asarray(cluster_vec))\n",
        "\n",
        "        top_idx = np.argsort(cluster_vec)[-top_n:][::-1]\n",
        "        top_terms = [(terms[i], float(cluster_vec[i])) for i in top_idx]\n",
        "        cluster_terms[cluster] = top_terms\n",
        "\n",
        "    return cluster_terms"
      ],
      "metadata": {
        "id": "deyAXdx7WPIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple, Optional\n",
        "import pandas as pd\n",
        "\n",
        "def print_cluster_summary(\n",
        "    df: pd.DataFrame,\n",
        "    labels: List[int],\n",
        "    cluster_terms: Dict[int, List[Tuple[str, float]]],\n",
        "    summaries: Optional[Dict[int, str]] = None,\n",
        "    top_docs_per_cluster: int = 5\n",
        "):\n",
        "    df = df.copy()\n",
        "    df['cluster'] = labels\n",
        "\n",
        "    for cluster in sorted(set(labels)):\n",
        "        cluster_df = df[df['cluster'] == cluster]\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Cluster {cluster} | {len(cluster_df)} docs\")\n",
        "\n",
        "\n",
        "        top_terms = cluster_terms.get(cluster, [])\n",
        "        label_terms = \", \".join([t for t, _ in top_terms[:6]]) if top_terms else \"N/A\"\n",
        "        print(f\"Cluster label (top terms): {label_terms}\")\n",
        "\n",
        "\n",
        "        if summaries and cluster in summaries:\n",
        "            print(f\"Cluster summary: {summaries[cluster]}\")\n",
        "\n",
        "        print(\"\\nTop documents (title + snippet):\")\n",
        "\n",
        "        for _, row in cluster_df.head(top_docs_per_cluster).iterrows():\n",
        "            snippet_words = row['clean_text'].split()\n",
        "            snippet = \" \".join(snippet_words[:35])\n",
        "            if len(snippet_words) > 35:\n",
        "                snippet += \"...\"\n",
        "\n",
        "            print(f\" - [{row['doc_id']}] {row['title']}\")\n",
        "            print(f\"   snippet: {snippet}\")\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "N5KNtnDfWSG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(\n",
        "    max_docs: int = 2000,\n",
        "    n_clusters: int = 10,\n",
        "    clustering_algo: str = 'kmeans',\n",
        "    dbscan_eps: float = 0.7,\n",
        "    dbscan_min_samples: int = 5,\n",
        "    tfidf_max_features: int = 10000,\n",
        "    random_state: int = 42,\n",
        "    summarize: bool = True,\n",
        "    use_sbert: bool = False\n",
        "):\n",
        "\n",
        "    df = load_reuters_dataset(max_docs=max_docs)\n",
        "    df = df[df['text'].str.len() > 50].reset_index(drop=True)\n",
        "    print(f\"After filtering short docs: {len(df)} documents\")\n",
        "\n",
        "\n",
        "    print(\"Preprocessing text (lowercase, tokenize, stopword removal, lemmatize)...\")\n",
        "    df = preprocess_dataframe(df, text_col='text', new_col='clean_text')\n",
        "\n",
        "\n",
        "    print(\"Vectorizing with TF-IDF...\")\n",
        "    tfidf_X, vectorizer = vectorize_tfidf(\n",
        "        df['clean_text'].tolist(),\n",
        "        max_features=tfidf_max_features\n",
        "    )\n",
        "\n",
        "\n",
        "    if use_sbert:\n",
        "        if not sbert_available:\n",
        "            print(\"sentence-transformers not installed; using TF-IDF embeddings.\")\n",
        "            embeddings = tfidf_X.toarray()\n",
        "        else:\n",
        "            print(\"Computing SBERT embeddings...\")\n",
        "            emb = sbert_embeddings(df['clean_text'].tolist())\n",
        "            embeddings = np.array(emb)\n",
        "    else:\n",
        "        embeddings = tfidf_X.toarray()\n",
        "\n",
        "\n",
        "    if clustering_algo == 'kmeans':\n",
        "        print(f\"Clustering with KMeans, k={n_clusters} ...\")\n",
        "        labels, model = cluster_kmeans(\n",
        "            embeddings,\n",
        "            k=n_clusters,\n",
        "            random_state=random_state\n",
        "        )\n",
        "    elif clustering_algo == 'dbscan':\n",
        "        print(f\"Clustering with DBSCAN, eps={dbscan_eps}, min_samples={dbscan_min_samples} ...\")\n",
        "        labels, model = cluster_dbscan(\n",
        "            embeddings,\n",
        "            eps=dbscan_eps,\n",
        "            min_samples=dbscan_min_samples\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"clustering_algo must be 'kmeans' or 'dbscan'\")\n",
        "\n",
        "\n",
        "    print(\"Generating cluster labels (top TF-IDF terms per cluster)...\")\n",
        "    cluster_terms = top_tfidf_terms_per_cluster(\n",
        "        tfidf_X,\n",
        "        labels,\n",
        "        vectorizer,\n",
        "        top_n=12\n",
        "    )\n",
        "\n",
        "\n",
        "    cluster_summaries = {}\n",
        "    if summarize:\n",
        "        print(\"Generating cluster summaries (TextRank / gensim)...\")\n",
        "        for cluster in sorted(set(labels)):\n",
        "            docs_for_summary = []\n",
        "            sub_df = df[labels == cluster]\n",
        "            for title, txt in zip(sub_df['title'], sub_df['text']):\n",
        "                docs_for_summary.append(title + \". \" + txt[:1000])\n",
        "\n",
        "            if len(docs_for_summary) == 0:\n",
        "                cluster_summaries[cluster] = \"\"\n",
        "            else:\n",
        "                cluster_summaries[cluster] = summarize_cluster_text(\n",
        "                    docs_for_summary,\n",
        "                    summary_sentences=2\n",
        "                )\n",
        "\n",
        "\n",
        "    print(\"Evaluating clustering...\")\n",
        "    eval_res = evaluate_clustering(\n",
        "        embeddings,\n",
        "        labels,\n",
        "        ground_truth=None\n",
        "    )\n",
        "    print(\"Evaluation metrics:\", eval_res)\n",
        "\n",
        "\n",
        "    print_cluster_summary(\n",
        "        df,\n",
        "        labels,\n",
        "        cluster_terms,\n",
        "        summaries=cluster_summaries,\n",
        "        top_docs_per_cluster=5\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Visualizing clusters (PCA then t-SNE)...\")\n",
        "    visualize_clusters(embeddings, labels, method='pca',\n",
        "                       title_suffix=f\"algo={clustering_algo}\")\n",
        "    visualize_clusters(embeddings, labels, method='tsne',\n",
        "                       title_suffix=f\"algo={clustering_algo}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        'df': df,\n",
        "        'labels': labels,\n",
        "        'cluster_terms': cluster_terms,\n",
        "        'cluster_summaries': cluster_summaries,\n",
        "        'eval': eval_res,\n",
        "        'model': model\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WI_3QZTRWYK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "LX7fLdukWvHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_reuters_dataset(max_docs: Optional[int] = 1000) -> pd.DataFrame:\n",
        "    fileids = reuters.fileids()\n",
        "    texts = []\n",
        "    for fid in fileids[:max_docs]:\n",
        "        raw = reuters.raw(fid)\n",
        "        sents = sent_tokenize(raw)\n",
        "        title = sents[0].strip() if sents else \"Reuters article \" + fid\n",
        "        if len(title.split()) > 12:\n",
        "            title = \" \".join(title.split()[:12]) + \"...\"\n",
        "        cats = reuters.categories(fid)\n",
        "        texts.append({'doc_id': fid, 'title': title, 'text': raw, 'categories': cats})\n",
        "    df = pd.DataFrame(texts)\n",
        "    print(f\"Loaded {len(df)} Reuters documents.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "TUnLPhw9W10u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text: str,\n",
        "                    remove_numbers: bool = True,\n",
        "                    remove_punct: bool = True,\n",
        "                    do_lemmatize: bool = True) -> str:\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "    if remove_numbers:\n",
        "        text = re.sub(r'\\d+', ' ', text)\n",
        "    if remove_punct:\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if len(t) > 2 and t not in STOPWORDS]\n",
        "    if do_lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame, text_col: str = 'text', new_col: str = 'clean_text') -> pd.DataFrame:\n",
        "    df[new_col] = df[text_col].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "def vectorize_tfidf(docs: List[str], max_features: int = 10000, ngram_range=(1,2)):\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "    X = vectorizer.fit_transform(docs)\n",
        "    return X, vectorizer\n",
        "\n",
        "def sbert_embeddings(docs: List[str], model_name: str = 'all-MiniLM-L6-v2', batch_size: int = 32):\n",
        "    if not sbert_available:\n",
        "        raise RuntimeError(\"sentence-transformers not available.\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(docs, show_progress_bar=True, batch_size=batch_size)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def cluster_kmeans(X, k: int = 10, random_state: int = 42):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    return kmeans.labels_, kmeans\n",
        "\n",
        "def cluster_dbscan(X, eps: float = 0.5, min_samples: int = 5, metric: str = 'cosine'):\n",
        "    if hasattr(X, \"toarray\"):\n",
        "        X_in = X.toarray()\n",
        "    else:\n",
        "        X_in = X\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
        "    labels = db.fit_predict(X_in)\n",
        "    return labels, db\n",
        "\n",
        "\n",
        "def top_tfidf_terms_per_cluster(tfidf_matrix, labels, vectorizer, top_n=8) -> Dict[int, List[Tuple[str, float]]]:\n",
        "    terms = np.array(vectorizer.get_feature_names_out())\n",
        "    labels = np.array(labels)\n",
        "    cluster_terms = {}\n",
        "    for cluster in sorted(set(labels)):\n",
        "        mask = (labels == cluster)\n",
        "        if np.sum(mask) == 0:\n",
        "            cluster_terms[cluster] = []\n",
        "            continue\n",
        "        cluster_vec = tfidf_matrix[mask].mean(axis=0)\n",
        "        if hasattr(cluster_vec, \"A1\"):\n",
        "            cluster_vec = np.squeeze(np.asarray(cluster_vec))\n",
        "        top_idx = np.argsort(cluster_vec)[-top_n:][::-1]\n",
        "        top_terms = [(terms[i], float(cluster_vec[i])) for i in top_idx]\n",
        "        cluster_terms[cluster] = top_terms\n",
        "    return cluster_terms\n",
        "\n",
        "\n",
        "def summarize_cluster_text(docs: List[str], summary_sentences: int = 2, ratio: Optional[float]=None) -> str:\n",
        "    combined = \"\\n\".join(docs)\n",
        "    try:\n",
        "        if ratio:\n",
        "            s = gensim_summarize(combined, ratio=ratio)\n",
        "        else:\n",
        "            approx_words = summary_sentences * 20\n",
        "            s = gensim_summarize(combined, word_count=approx_words)\n",
        "        if not s or len(s.strip()) == 0:\n",
        "            raise ValueError(\"Empty summary\")\n",
        "        return s.replace(\"\\n\", \" \")\n",
        "    except Exception:\n",
        "        sents = sent_tokenize(combined)\n",
        "        return \" \".join(sents[:summary_sentences]) if len(sents) >= summary_sentences else \" \".join(sents)\n",
        "\n",
        "\n",
        "def evaluate_clustering(embeddings_or_X, labels, ground_truth: Optional[List]=None) -> Dict[str, float]:\n",
        "    res = {}\n",
        "    try:\n",
        "        if len(set(labels)) > 1 and len(labels) > len(set(labels)):\n",
        "            res['silhouette'] = float(silhouette_score(embeddings_or_X, labels))\n",
        "        else:\n",
        "            res['silhouette'] = float('nan')\n",
        "    except:\n",
        "        res['silhouette'] = float('nan')\n",
        "    try:\n",
        "        db = davies_bouldin_score(embeddings_or_X if not hasattr(embeddings_or_X,\"toarray\") else embeddings_or_X.toarray(), labels)\n",
        "        res['davies_bouldin'] = float(db)\n",
        "    except:\n",
        "        res['davies_bouldin'] = float('nan')\n",
        "    if ground_truth is not None:\n",
        "        try:\n",
        "            nmi = normalized_mutual_info_score(ground_truth, labels)\n",
        "            res['NMI'] = float(nmi)\n",
        "        except:\n",
        "            res['NMI'] = float('nan')\n",
        "    return res"
      ],
      "metadata": {
        "id": "6ryYwhJgW44Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_pipeline(\n",
        "    max_docs=600,\n",
        "    n_clusters=12,\n",
        "    clustering_algo='kmeans',\n",
        "    summarize=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "4MdxuTN6XF3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}